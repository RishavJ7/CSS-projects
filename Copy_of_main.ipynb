{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyUsz7ugJ59IEP6GnvCpib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RishavJ7/CSS-projects/blob/main/Copy_of_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8U7Dfhzs36gU",
        "outputId": "f2203bcb-d18b-49d6-f988-d67f69482c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found 2523 images belonging to 3 classes.\n",
            "Found 629 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 2s/step - accuracy: 0.4387 - loss: 1.4648 - val_accuracy: 0.8760 - val_loss: 3.8015 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 3s/step - accuracy: 0.6569 - loss: 0.8740 - val_accuracy: 0.8760 - val_loss: 1.7451 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 3s/step - accuracy: 0.7854 - loss: 0.7181 - val_accuracy: 0.8760 - val_loss: 1.2178 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 2s/step - accuracy: 0.8178 - loss: 0.6262 - val_accuracy: 0.8760 - val_loss: 0.8653 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 3s/step - accuracy: 0.8347 - loss: 0.5925 - val_accuracy: 0.8760 - val_loss: 0.7161 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 2s/step - accuracy: 0.8438 - loss: 0.5598 - val_accuracy: 0.8760 - val_loss: 0.5100 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 2s/step - accuracy: 0.8561 - loss: 0.5142 - val_accuracy: 0.8760 - val_loss: 0.5014 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 3s/step - accuracy: 0.8557 - loss: 0.4954 - val_accuracy: 0.8760 - val_loss: 0.4638 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 3s/step - accuracy: 0.8556 - loss: 0.5200 - val_accuracy: 0.8760 - val_loss: 0.4599 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.8623 - loss: 0.4870 - val_accuracy: 0.8760 - val_loss: 0.4828 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 3s/step - accuracy: 0.8705 - loss: 0.4769 - val_accuracy: 0.8760 - val_loss: 0.5015 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 3s/step - accuracy: 0.8709 - loss: 0.4555 - val_accuracy: 0.8760 - val_loss: 0.4493 - learning_rate: 5.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 2s/step - accuracy: 0.8602 - loss: 0.4550 - val_accuracy: 0.8760 - val_loss: 0.4707 - learning_rate: 5.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 3s/step - accuracy: 0.8733 - loss: 0.4275 - val_accuracy: 0.8760 - val_loss: 0.4351 - learning_rate: 5.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 3s/step - accuracy: 0.8677 - loss: 0.4408 - val_accuracy: 0.8760 - val_loss: 0.4515 - learning_rate: 5.0000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 2s/step - accuracy: 0.8696 - loss: 0.4521 - val_accuracy: 0.8760 - val_loss: 0.4101 - learning_rate: 5.0000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 3s/step - accuracy: 0.8576 - loss: 0.4592 - val_accuracy: 0.8760 - val_loss: 0.4151 - learning_rate: 5.0000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 2s/step - accuracy: 0.8624 - loss: 0.4430 - val_accuracy: 0.8760 - val_loss: 0.4140 - learning_rate: 5.0000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 2s/step - accuracy: 0.8629 - loss: 0.4202 - val_accuracy: 0.8760 - val_loss: 0.4066 - learning_rate: 2.5000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 3s/step - accuracy: 0.8591 - loss: 0.4347 - val_accuracy: 0.8760 - val_loss: 0.4111 - learning_rate: 2.5000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 2s/step - accuracy: 0.8674 - loss: 0.4232 - val_accuracy: 0.8744 - val_loss: 0.4152 - learning_rate: 2.5000e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 2s/step - accuracy: 0.8630 - loss: 0.4404 - val_accuracy: 0.8760 - val_loss: 0.3980 - learning_rate: 1.2500e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 3s/step - accuracy: 0.8457 - loss: 0.4589 - val_accuracy: 0.8760 - val_loss: 0.4148 - learning_rate: 1.2500e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 2s/step - accuracy: 0.8652 - loss: 0.4205 - val_accuracy: 0.8776 - val_loss: 0.4122 - learning_rate: 1.2500e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 2s/step - accuracy: 0.8826 - loss: 0.3850 - val_accuracy: 0.8760 - val_loss: 0.4020 - learning_rate: 6.2500e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 2s/step - accuracy: 0.8681 - loss: 0.4301 - val_accuracy: 0.8760 - val_loss: 0.3901 - learning_rate: 6.2500e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 3s/step - accuracy: 0.8677 - loss: 0.4218 - val_accuracy: 0.8760 - val_loss: 0.4108 - learning_rate: 6.2500e-05\n",
            "Epoch 28/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 2s/step - accuracy: 0.8771 - loss: 0.3975 - val_accuracy: 0.8760 - val_loss: 0.4147 - learning_rate: 6.2500e-05\n",
            "Epoch 29/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 3s/step - accuracy: 0.8758 - loss: 0.3921 - val_accuracy: 0.8760 - val_loss: 0.3951 - learning_rate: 3.1250e-05\n",
            "Epoch 30/30\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 2s/step - accuracy: 0.8621 - loss: 0.4290 - val_accuracy: 0.8760 - val_loss: 0.4006 - learning_rate: 3.1250e-05\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 613ms/step - accuracy: 0.8485 - loss: 0.5265\n",
            "✅ Validation Accuracy: 0.8759936690330505\n",
            "❌ Validation Loss: 0.3997098505496979\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-48bcaa61c9b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# === 🗂️ Mount Google Drive ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === 📦 Unzip Dataset ===\n",
        "!unzip -q \"/content/drive/MyDrive/Copy of dry_oily_normal.zip\" -d \"/content/dataset\"\n",
        "\n",
        "# === 📁 Set Parameters ===\n",
        "dataset_path = \"/content/dataset/Oily-Dry-Skin-Types\"  # Adjust folder name if needed\n",
        "img_size = (128, 128)\n",
        "batch_size = 32\n",
        "\n",
        "# === 🧪 Data Augmentation and Preprocessing ===\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# === 📥 Load Train/Val Data ===\n",
        "train_data = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_data = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# === 🧠 CNN Model ===\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(img_size[0], img_size[1], 3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(train_data.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# === ⚙️ Compile Model ===\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# === ⛔ Callbacks to Prevent Overfitting ===\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
        "\n",
        "# === 🚀 Train Model ===\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=30,\n",
        "    validation_data=val_data,\n",
        "    callbacks=[early_stop, reduce_lr]\n",
        ")\n",
        "\n",
        "# === 📊 Evaluate Model ===\n",
        "val_loss, val_accuracy = model.evaluate(val_data)\n",
        "print(\"✅ Validation Accuracy:\", val_accuracy)\n",
        "print(\"❌ Validation Loss:\", val_loss)\n",
        "\n",
        "# === 🔍 Scan and Predict Image ===\n",
        "img_path = \"/content/dataset/Oily-Dry-Skin-Types/test/oily/100.jpg\"  # Change if needed\n",
        "\n",
        "img = cv2.imread(img_path)\n",
        "img = cv2.resize(img, img_size)\n",
        "img = img.astype('float32') / 255.0\n",
        "img = np.expand_dims(img, axis=0)\n",
        "\n",
        "prediction = model.predict(img)\n",
        "predicted_class = np.argmax(prediction)\n",
        "class_labels = list(train_data.class_indices.keys())\n",
        "\n",
        "print(\"🔮 Predicted Skin Type:\", class_labels[predicted_class])\n"
      ]
    }
  ]
}